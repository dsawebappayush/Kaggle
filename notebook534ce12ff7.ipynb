{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8732277,"sourceType":"datasetVersion","datasetId":5241531},{"sourceId":8757543,"sourceType":"datasetVersion","datasetId":5261218},{"sourceId":8759694,"sourceType":"datasetVersion","datasetId":5262822},{"sourceId":8759733,"sourceType":"datasetVersion","datasetId":5262852},{"sourceId":9514166,"sourceType":"datasetVersion","datasetId":5791774},{"sourceId":184902460,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random","metadata":{"id":"sb0zW1T7DIGh","execution":{"iopub.status.busy":"2024-09-30T14:47:50.341542Z","iopub.execute_input":"2024-09-30T14:47:50.341821Z","iopub.status.idle":"2024-09-30T14:47:50.351898Z","shell.execute_reply.started":"2024-09-30T14:47:50.341789Z","shell.execute_reply":"2024-09-30T14:47:50.350929Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:47:50.353773Z","iopub.execute_input":"2024-09-30T14:47:50.354095Z","iopub.status.idle":"2024-09-30T14:47:50.361959Z","shell.execute_reply.started":"2024-09-30T14:47:50.354061Z","shell.execute_reply":"2024-09-30T14:47:50.361108Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle\n","output_type":"stream"}]},{"cell_type":"code","source":"cd ..\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:47:50.362917Z","iopub.execute_input":"2024-09-30T14:47:50.363188Z","iopub.status.idle":"2024-09-30T14:47:50.370041Z","shell.execute_reply.started":"2024-09-30T14:47:50.363152Z","shell.execute_reply":"2024-09-30T14:47:50.369149Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:47:50.371957Z","iopub.execute_input":"2024-09-30T14:47:50.372545Z","iopub.status.idle":"2024-09-30T14:47:51.396017Z","shell.execute_reply.started":"2024-09-30T14:47:50.372512Z","shell.execute_reply":"2024-09-30T14:47:51.394718Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"'=24.4'                                     \u001b[0m\u001b[01;36mlibx32\u001b[0m@\n NGC-DL-CONTAINER-LICENSE                   \u001b[01;34mmedia\u001b[0m/\n \u001b[01;36mbin\u001b[0m@                                       \u001b[01;34mmnt\u001b[0m/\n \u001b[01;34mboot\u001b[0m/                                      \u001b[01;34mopt\u001b[0m/\n cuda-keyring_1.0-1_all.deb                 \u001b[01;34mproc\u001b[0m/\n \u001b[01;34mdev\u001b[0m/                                       \u001b[01;34mroot\u001b[0m/\n \u001b[01;32mentrypoint.sh\u001b[0m*                             \u001b[01;34mrun\u001b[0m/\n \u001b[01;34metc\u001b[0m/                                       \u001b[01;32mrun_jupyter.sh\u001b[0m*\n \u001b[01;34mhome\u001b[0m/                                      \u001b[01;36msbin\u001b[0m@\n \u001b[01;32minstall_packages.sh\u001b[0m*                       \u001b[01;34msrv\u001b[0m/\n \u001b[01;34mkaggle\u001b[0m/                                    \u001b[01;34msys\u001b[0m/\n \u001b[01;36mlib\u001b[0m@                                       \u001b[30;42mtmp\u001b[0m/\n \u001b[01;36mlib32\u001b[0m@                                     \u001b[01;34musr\u001b[0m/\n \u001b[01;36mlib64\u001b[0m@                                     \u001b[01;34mvar\u001b[0m/\n libnvinfer8_8.6.1.6-1+cuda12.0_amd64.deb\n","output_type":"stream"}]},{"cell_type":"code","source":"### read words ###\ntext_file = open('kaggle/input/dataset/words_250000_train.txt',\"r\")\nfull_dictionary = text_file.read().splitlines()\n","metadata":{"id":"U4mKEJSODK3p","execution":{"iopub.status.busy":"2024-09-30T14:47:51.398154Z","iopub.execute_input":"2024-09-30T14:47:51.398895Z","iopub.status.idle":"2024-09-30T14:47:51.470018Z","shell.execute_reply.started":"2024-09-30T14:47:51.398844Z","shell.execute_reply":"2024-09-30T14:47:51.460770Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-09-30T14:47:51.472824Z","iopub.execute_input":"2024-09-30T14:47:51.473320Z","iopub.status.idle":"2024-09-30T14:47:51.482980Z","shell.execute_reply.started":"2024-09-30T14:47:51.473273Z","shell.execute_reply":"2024-09-30T14:47:51.481954Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\nrandom.seed(40)\n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(40)\n\n# Sample data for demonstration\nwords = full_dictionary\n\ndef generate_training_data_3(words):\n    X = []\n    y = []\n    for word in words:\n      # print(word)\n      for char in np.unique(list(word)):\n          X.append(word.replace(char,\"_\"))  # Input with missing character\n          y.append(char)\n          try:\n            X.append(word.replace(char,\"_\").replace(random.choice(list(word.replace(char,\"\"))),\"_\"))  # Input with missing character\n            y.append(char)                      # Target character (the missing one)\n          except:\n            pass\n          # print(word.replace(char,\"_\"))\n    return X, y\n\n\n# Toy dataset\nX_train_3, y_train_3 = generate_training_data_3(words)\n\nwords_3 = X_train_3\nlabels_3 = y_train_3\n\nmax_length_3 = max(len(word) for word in words_3)\n\n# Create a dictionary for character to index mapping\nchar_to_index = {char: idx for idx, char in enumerate(set(''.join(words_3)))}\nindex_to_char = {idx: char for char, idx in char_to_index.items()}\n\n# Convert words and labels to numerical format\nX_padded_3 = pad_sequences([[char_to_index[char] for char in word] for word in words_3], maxlen=max_length_3, padding='post')\ny_padded_3 = np.array([char_to_index[label] for label in labels_3])\n\nX_train_3, X_temp_3, y_train_3, y_temp_3 = train_test_split(X_padded_3, y_padded_3, test_size=0.3, random_state=42)\nX_val_3, X_test_3, y_val_3, y_test_3 = train_test_split(X_temp_3, y_temp_3, test_size=0.5, random_state=42)\n\n# print(\"Padded data:\")\n# for i in range(len(words)):\n#     print(f\"{words[i]} -> {labels[i]}\")\n\n# print(\"\\nPadded and numerical data:\")\n# print(X_padded_2)\n# print(y_padded_2)\n","metadata":{"id":"uBBtPhXtrc6Q","execution":{"iopub.status.busy":"2024-09-30T14:47:51.486349Z","iopub.execute_input":"2024-09-30T14:47:51.486756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BiLSTM with attention\n\n## attention \n\n## LSTM 256\n\n## embedding - 50\n\n## no dropout","metadata":{"execution":{"iopub.execute_input":"2024-06-20T18:25:02.064844Z","iopub.status.busy":"2024-06-20T18:25:02.063934Z","iopub.status.idle":"2024-06-20T18:25:02.070731Z","shell.execute_reply":"2024-06-20T18:25:02.069350Z","shell.execute_reply.started":"2024-06-20T18:25:02.064810Z"}}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define a checkpoint callback\ncheckpoint_callback_6 = ModelCheckpoint('best_model_6.keras', \n                                      monitor='val_loss', \n                                      save_best_only=True, \n                                      mode='min', \n                                      verbose=1)\n\nearly_stopping_callback = EarlyStopping(monitor='val_loss', \n                                       patience=3, \n                                       mode='min', \n                                       verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# # instantiate a distribution strategy\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, Layer\nfrom tensorflow.keras import Input, Model\nimport keras\n# Define parameters\nvocab_size = len(char_to_index)  # Size of vocabulary (unique characters)\nembedding_dim = 50  # Dimension of character embeddings\nlstm_units = 256  # Number of units in LSTM layers\nnum_layers = 3  # Number of BiLSTM layers\ndropout_rate = 0.1  # Dropout rate\n\n# Custom Attention Layer\n@keras.saving.register_keras_serializable()\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        et = tf.squeeze(tf.tanh(tf.matmul(x, self.W) + self.b), axis=-1)\n        at = tf.nn.softmax(et)\n        at = tf.expand_dims(at, axis=-1)\n        output = x * at\n        return tf.reduce_sum(output, axis=1)\n\n# Define Sequential model\nmodel_6 = Sequential()\n\n# Add layers to the model\nmodel_6.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length_3))\n\n# Add BiLSTM layers with Dropout\nfor _ in range(num_layers):\n    model_6.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n    model_6.add(Dropout(rate=dropout_rate))\n\n# Add Attention Layer\nmodel_6.add(AttentionLayer())\n\n# Output layer\nmodel_6.add(Dense(units=vocab_size, activation='softmax'))\n\n# Compile the model\nmodel_6.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_6.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit( X_train_3, \n            y_train_3, \n            epochs=3, \n            batch_size=256, \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit( X_train_3, \n            y_train_3, \n            epochs=7, \n            batch_size=256, \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint_callback_total6 = ModelCheckpoint('best_model_6.keras', \n#                                       monitor='val_loss', \n#                                       save_best_only=True, \n#                                       mode='min', \n#                                       verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit(X_train_3, \n            y_train_3, \n            epochs=3,      \n            batch_size=256,                     \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,\n            callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Complete Training data\n# randome shuffling ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, Layer\nfrom tensorflow.keras import Input, Model\nimport keras\n# Define parameters\nvocab_size = len(char_to_index)  # Size of vocabulary (unique characters)\nembedding_dim = 50  # Dimension of character embeddings\nlstm_units = 256  # Number of units in LSTM layers\nnum_layers = 3  # Number of BiLSTM layers\ndropout_rate = 0.1  # Dropout rate\n\n# Custom Attention Layer\n@keras.saving.register_keras_serializable()\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        et = tf.squeeze(tf.tanh(tf.matmul(x, self.W) + self.b), axis=-1)\n        at = tf.nn.softmax(et)\n        at = tf.expand_dims(at, axis=-1)\n        output = x * at\n        return tf.reduce_sum(output, axis=1)\n\n# Define Sequential model\nmodel_complete_6 = Sequential()\n\n# Add layers to the model\nmodel_complete_6.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length_3))\n\n# Add BiLSTM layers with Dropout\nfor _ in range(num_layers):\n    model_complete_6.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n    model_complete_6.add(Dropout(rate=dropout_rate))\n\n# Add Attention Layer\nmodel_complete_6.add(AttentionLayer())\n\n# Output layer\nmodel_complete_6.add(Dense(units=vocab_size, activation='softmax'))\n\n# Compile the model\nmodel_complete_6.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_complete_6.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\nX_train_complete_3, y_train_complete_3 = shuffle( X_padded_3, y_padded_3 )\n\n# Train the model\nmodel_complete_6.fit( X_train_complete_3, \n            y_train_complete_3,\n            epochs=8, \n            batch_size=256, \n#             validation_data=(X_val_3, y_val_3), \n            verbose=1,\n#             callbacks=[checkpoint_callback_6,early_stopping_callback]\n           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_complete_6.save('best_model_6_complete.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_complete_6_recon = tf.keras.models.load_model('best_model_6_complete.keras')\nfrom IPython.display import FileLink\nFileLink(r'best_model_6_complete.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_complete_6_recon = tf.keras.models.load_model('best_model_6_complete.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## load model \n# model_6_recon = tf.keras.models.load_model('best_model_6.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on test set\nloss, accuracy = model_complete_6_recon.evaluate(X_test_3, y_test_3, verbose=1)\nprint(f\"validation set Accuracy: {accuracy*100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}