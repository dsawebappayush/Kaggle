{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8732277,"sourceType":"datasetVersion","datasetId":5241531},{"sourceId":8757543,"sourceType":"datasetVersion","datasetId":5261218},{"sourceId":8759694,"sourceType":"datasetVersion","datasetId":5262822},{"sourceId":8759733,"sourceType":"datasetVersion","datasetId":5262852},{"sourceId":9514166,"sourceType":"datasetVersion","datasetId":5791774},{"sourceId":184902460,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random","metadata":{"id":"sb0zW1T7DIGh","execution":{"iopub.status.busy":"2024-09-30T09:05:16.223956Z","iopub.execute_input":"2024-09-30T09:05:16.224300Z","iopub.status.idle":"2024-09-30T09:05:16.234242Z","shell.execute_reply.started":"2024-09-30T09:05:16.224261Z","shell.execute_reply":"2024-09-30T09:05:16.233184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:16.236234Z","iopub.execute_input":"2024-09-30T09:05:16.236862Z","iopub.status.idle":"2024-09-30T09:05:16.245012Z","shell.execute_reply.started":"2024-09-30T09:05:16.236818Z","shell.execute_reply":"2024-09-30T09:05:16.244174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ..\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:16.246151Z","iopub.execute_input":"2024-09-30T09:05:16.246458Z","iopub.status.idle":"2024-09-30T09:05:16.256420Z","shell.execute_reply.started":"2024-09-30T09:05:16.246426Z","shell.execute_reply":"2024-09-30T09:05:16.255449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:16.257752Z","iopub.execute_input":"2024-09-30T09:05:16.258155Z","iopub.status.idle":"2024-09-30T09:05:17.263273Z","shell.execute_reply.started":"2024-09-30T09:05:16.258114Z","shell.execute_reply":"2024-09-30T09:05:17.262265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### read words ###\ntext_file = open('kaggle/input/dataset/words_250000_train.txt',\"r\")\nfull_dictionary = text_file.read().splitlines()\n","metadata":{"id":"U4mKEJSODK3p","execution":{"iopub.status.busy":"2024-09-30T09:05:17.266459Z","iopub.execute_input":"2024-09-30T09:05:17.266770Z","iopub.status.idle":"2024-09-30T09:05:17.321750Z","shell.execute_reply.started":"2024-09-30T09:05:17.266737Z","shell.execute_reply":"2024-09-30T09:05:17.320728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:17.322856Z","iopub.execute_input":"2024-09-30T09:05:17.323207Z","iopub.status.idle":"2024-09-30T09:05:17.329246Z","shell.execute_reply.started":"2024-09-30T09:05:17.323173Z","shell.execute_reply":"2024-09-30T09:05:17.328283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nrandom.seed(40)\n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(40)\n\n# Sample data for demonstration\nwords = full_dictionary\n\ndef generate_training_data_3(words):\n    X = []\n    y = []\n    for word in words:\n      # print(word)\n      for char in np.unique(list(word)):\n          X.append(word.replace(char,\"_\"))  # Input with missing character\n          y.append(char)\n          try:\n            X.append(word.replace(char,\"_\").replace(random.choice(list(word.replace(char,\"\"))),\"_\"))  # Input with missing character\n            y.append(char)                      # Target character (the missing one)\n          except:\n            pass\n          # print(word.replace(char,\"_\"))\n    return X, y\n\n\n# Toy dataset\nX_train_3, y_train_3 = generate_training_data_3(words)\n\nwords_3 = X_train_3\nlabels_3 = y_train_3\n\nmax_length_3 = max(len(word) for word in words_3)\n\n# Create a dictionary for character to index mapping\nchar_to_index = {char: idx for idx, char in enumerate(set(''.join(words_3)))}\nindex_to_char = {idx: char for char, idx in char_to_index.items()}\n\n# Convert words and labels to numerical format\nX_padded_3 = pad_sequences([[char_to_index[char] for char in word] for word in words_3], maxlen=max_length_3, padding='post')\ny_padded_3 = np.array([char_to_index[label] for label in labels_3])\n\nX_train_3, X_temp_3, y_train_3, y_temp_3 = train_test_split(X_padded_3, y_padded_3, test_size=0.3, random_state=42)\nX_val_3, X_test_3, y_val_3, y_test_3 = train_test_split(X_temp_3, y_temp_3, test_size=0.5, random_state=42)\n\n# print(\"Padded data:\")\n# for i in range(len(words)):\n#     print(f\"{words[i]} -> {labels[i]}\")\n\n# print(\"\\nPadded and numerical data:\")\n# print(X_padded_2)\n# print(y_padded_2)\n","metadata":{"id":"uBBtPhXtrc6Q","execution":{"iopub.status.busy":"2024-09-30T09:05:17.330530Z","iopub.execute_input":"2024-09-30T09:05:17.331217Z","iopub.status.idle":"2024-09-30T09:05:23.127116Z","shell.execute_reply.started":"2024-09-30T09:05:17.331174Z","shell.execute_reply":"2024-09-30T09:05:23.124923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BiLSTM with attention\n\n## attention \n\n## LSTM 256\n\n## embedding - 50\n\n## no dropout","metadata":{"execution":{"iopub.execute_input":"2024-06-20T18:25:02.064844Z","iopub.status.busy":"2024-06-20T18:25:02.063934Z","iopub.status.idle":"2024-06-20T18:25:02.070731Z","shell.execute_reply":"2024-06-20T18:25:02.069350Z","shell.execute_reply.started":"2024-06-20T18:25:02.064810Z"}}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define a checkpoint callback\ncheckpoint_callback_6 = ModelCheckpoint('best_model_6.keras', \n                                      monitor='val_loss', \n                                      save_best_only=True, \n                                      mode='min', \n                                      verbose=1)\n\nearly_stopping_callback = EarlyStopping(monitor='val_loss', \n                                       patience=3, \n                                       mode='min', \n                                       verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.128173Z","iopub.status.idle":"2024-09-30T09:05:23.128567Z","shell.execute_reply.started":"2024-09-30T09:05:23.128380Z","shell.execute_reply":"2024-09-30T09:05:23.128400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n# # instantiate a distribution strategy\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\n# # instantiating the model in the strategy scope creates the model on the TPU","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.130259Z","iopub.status.idle":"2024-09-30T09:05:23.130640Z","shell.execute_reply.started":"2024-09-30T09:05:23.130454Z","shell.execute_reply":"2024-09-30T09:05:23.130473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, Layer\nfrom tensorflow.keras import Input, Model\nimport keras\n# Define parameters\nvocab_size = len(char_to_index)  # Size of vocabulary (unique characters)\nembedding_dim = 50  # Dimension of character embeddings\nlstm_units = 256  # Number of units in LSTM layers\nnum_layers = 3  # Number of BiLSTM layers\ndropout_rate = 0.1  # Dropout rate\n\n# Custom Attention Layer\n@keras.saving.register_keras_serializable()\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        et = tf.squeeze(tf.tanh(tf.matmul(x, self.W) + self.b), axis=-1)\n        at = tf.nn.softmax(et)\n        at = tf.expand_dims(at, axis=-1)\n        output = x * at\n        return tf.reduce_sum(output, axis=1)\n\n# Define Sequential model\nmodel_6 = Sequential()\n\n# Add layers to the model\nmodel_6.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length_3))\n\n# Add BiLSTM layers with Dropout\nfor _ in range(num_layers):\n    model_6.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n    model_6.add(Dropout(rate=dropout_rate))\n\n# Add Attention Layer\nmodel_6.add(AttentionLayer())\n\n# Output layer\nmodel_6.add(Dense(units=vocab_size, activation='softmax'))\n\n# Compile the model\nmodel_6.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_6.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.132541Z","iopub.status.idle":"2024-09-30T09:05:23.132952Z","shell.execute_reply.started":"2024-09-30T09:05:23.132726Z","shell.execute_reply":"2024-09-30T09:05:23.132762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit( X_train_3, \n            y_train_3, \n            epochs=3, \n            batch_size=256, \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.134346Z","iopub.status.idle":"2024-09-30T09:05:23.135108Z","shell.execute_reply.started":"2024-09-30T09:05:23.134910Z","shell.execute_reply":"2024-09-30T09:05:23.134938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit( X_train_3, \n            y_train_3, \n            epochs=7, \n            batch_size=256, \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.136441Z","iopub.status.idle":"2024-09-30T09:05:23.136796Z","shell.execute_reply.started":"2024-09-30T09:05:23.136620Z","shell.execute_reply":"2024-09-30T09:05:23.136638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint_callback_total6 = ModelCheckpoint('best_model_6.keras', \n#                                       monitor='val_loss', \n#                                       save_best_only=True, \n#                                       mode='min', \n#                                       verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.138376Z","iopub.status.idle":"2024-09-30T09:05:23.138743Z","shell.execute_reply.started":"2024-09-30T09:05:23.138565Z","shell.execute_reply":"2024-09-30T09:05:23.138583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel_6.fit(X_train_3, \n            y_train_3, \n            epochs=3,      \n            batch_size=256,                     \n            validation_data=(X_val_3, y_val_3), \n            verbose=1,\n            callbacks=[checkpoint_callback_6,early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.139921Z","iopub.status.idle":"2024-09-30T09:05:23.140283Z","shell.execute_reply.started":"2024-09-30T09:05:23.140109Z","shell.execute_reply":"2024-09-30T09:05:23.140127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Complete Training data\n# randome shuffling ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate, Layer\nfrom tensorflow.keras import Input, Model\nimport keras\n# Define parameters\nvocab_size = len(char_to_index)  # Size of vocabulary (unique characters)\nembedding_dim = 50  # Dimension of character embeddings\nlstm_units = 256  # Number of units in LSTM layers\nnum_layers = 3  # Number of BiLSTM layers\ndropout_rate = 0.1  # Dropout rate\n\n# Custom Attention Layer\n@keras.saving.register_keras_serializable()\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, x):\n        et = tf.squeeze(tf.tanh(tf.matmul(x, self.W) + self.b), axis=-1)\n        at = tf.nn.softmax(et)\n        at = tf.expand_dims(at, axis=-1)\n        output = x * at\n        return tf.reduce_sum(output, axis=1)\n\n# Define Sequential model\nmodel_complete_6 = Sequential()\n\n# Add layers to the model\nmodel_complete_6.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length_3))\n\n# Add BiLSTM layers with Dropout\nfor _ in range(num_layers):\n    model_complete_6.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n    model_complete_6.add(Dropout(rate=dropout_rate))\n\n# Add Attention Layer\nmodel_complete_6.add(AttentionLayer())\n\n# Output layer\nmodel_complete_6.add(Dense(units=vocab_size, activation='softmax'))\n\n# Compile the model\nmodel_complete_6.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_complete_6.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.141388Z","iopub.status.idle":"2024-09-30T09:05:23.141740Z","shell.execute_reply.started":"2024-09-30T09:05:23.141561Z","shell.execute_reply":"2024-09-30T09:05:23.141578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\nX_train_complete_3, y_train_complete_3 = shuffle( X_padded_3, y_padded_3 )\n\n# Train the model\nmodel_complete_6.fit( X_train_complete_3, \n            y_train_complete_3,\n            epochs=8, \n            batch_size=256, \n#             validation_data=(X_val_3, y_val_3), \n            verbose=1,\n#             callbacks=[checkpoint_callback_6,early_stopping_callback]\n           )","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.142958Z","iopub.status.idle":"2024-09-30T09:05:23.143309Z","shell.execute_reply.started":"2024-09-30T09:05:23.143134Z","shell.execute_reply":"2024-09-30T09:05:23.143152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_complete_6.save('best_model_6_complete.keras')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.145451Z","iopub.status.idle":"2024-09-30T09:05:23.146134Z","shell.execute_reply.started":"2024-09-30T09:05:23.145912Z","shell.execute_reply":"2024-09-30T09:05:23.145942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_complete_6_recon = tf.keras.models.load_model('best_model_6_complete.keras')\nfrom IPython.display import FileLink\nFileLink(r'best_model_6_complete.keras')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.147321Z","iopub.status.idle":"2024-09-30T09:05:23.147676Z","shell.execute_reply.started":"2024-09-30T09:05:23.147501Z","shell.execute_reply":"2024-09-30T09:05:23.147519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_complete_6_recon = tf.keras.models.load_model('best_model_6_complete.keras')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.149098Z","iopub.status.idle":"2024-09-30T09:05:23.149427Z","shell.execute_reply.started":"2024-09-30T09:05:23.149260Z","shell.execute_reply":"2024-09-30T09:05:23.149277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## load model \n# model_6_recon = tf.keras.models.load_model('best_model_6.keras')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.151288Z","iopub.status.idle":"2024-09-30T09:05:23.151651Z","shell.execute_reply.started":"2024-09-30T09:05:23.151474Z","shell.execute_reply":"2024-09-30T09:05:23.151493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on test set\nloss, accuracy = model_complete_6_recon.evaluate(X_test_3, y_test_3, verbose=1)\nprint(f\"validation set Accuracy: {accuracy*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.152957Z","iopub.status.idle":"2024-09-30T09:05:23.153306Z","shell.execute_reply.started":"2024-09-30T09:05:23.153132Z","shell.execute_reply":"2024-09-30T09:05:23.153151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-30T09:05:23.154845Z","iopub.status.idle":"2024-09-30T09:05:23.155227Z","shell.execute_reply.started":"2024-09-30T09:05:23.155050Z","shell.execute_reply":"2024-09-30T09:05:23.155069Z"},"trusted":true},"execution_count":null,"outputs":[]}]}